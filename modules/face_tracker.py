#!/usr/bin/env python3\n\"\"\"\nFace Tracker Module\nSmart face detection dan tracking untuk mendeteksi wajah, tracking pergerakan,\ndan mengidentifikasi siapa yang sedang aktif di video\n\"\"\"\n\nimport cv2\nimport numpy as np\nimport face_recognition\nimport torch\nfrom pathlib import Path\nimport logging\nfrom typing import List, Dict, Tuple, Optional\nfrom dataclasses import dataclass\nimport pickle\nimport json\nfrom moviepy.editor import VideoFileClip\nfrom collections import defaultdict, deque\nimport math\n\n# Setup logging\nlogging.basicConfig(level=logging.INFO)\nlogger = logging.getLogger(__name__)\n\n@dataclass\nclass FaceDetection:\n    \"\"\"Data class untuk face detection results\"\"\"\n    timestamp: float\n    face_id: int\n    confidence: float\n    bounding_box: Tuple[int, int, int, int]  # (x, y, width, height)\n    landmarks: Optional[List[Tuple[int, int]]]\n    encoding: Optional[np.ndarray]\n    size: float  # Relative size of face\n    center: Tuple[int, int]\n    \n@dataclass\nclass FaceTrack:\n    \"\"\"Data class untuk face tracking across time\"\"\"\n    face_id: int\n    first_seen: float\n    last_seen: float\n    total_duration: float\n    appearances: int\n    average_size: float\n    average_confidence: float\n    face_encoding: np.ndarray\n    track_history: List[FaceDetection]\n    is_main_speaker: bool = False\n    \nclass FaceTracker:\n    def __init__(self, models_dir=None):\n        \"\"\"Initialize face tracker\"\"\"\n        self.models_dir = Path(models_dir) if models_dir else Path(__file__).parent.parent / \"models\"\n        self.models_dir.mkdir(exist_ok=True)\n        \n        # Face detection parameters\n        self.face_detection_model = 'hog'  # 'hog' untuk CPU, 'cnn' untuk GPU\n        self.face_recognition_tolerance = 0.6\n        self.min_face_size = 0.02  # Minimum 2% of frame area\n        self.confidence_threshold = 0.5\n        \n        # Tracking parameters\n        self.max_face_distance = 0.5  # For face matching across frames\n        self.track_timeout = 5.0  # Seconds before track expires\n        self.sample_rate = 2.0  # Process every 2 seconds\n        \n        # Initialize trackers\n        self.face_tracks = {}\n        self.next_face_id = 0\n        self.known_faces = {}  # For pre-registered faces\n        \n        # GPU detection if available\n        if torch.cuda.is_available():\n            self.face_detection_model = 'cnn'\n            logger.info(\"Using GPU for face detection\")\n        else:\n            logger.info(\"Using CPU for face detection\")\n            \n    def track_faces(self, video_path, progress_callback=None):\n        \"\"\"\n        Main function untuk tracking faces dalam video\n        \n        Args:\n            video_path: Path ke video file\n            progress_callback: Function untuk progress updates\n            \n        Returns:\n            Dict dengan face tracking results\n        \"\"\"\n        try:\n            logger.info(f\"Starting face tracking: {video_path}\")\n            \n            # Load video\n            video = VideoFileClip(video_path)\n            duration = video.duration\n            fps = video.fps\n            \n            # Reset tracking state\n            self.face_tracks = {}\n            self.next_face_id = 0\n            \n            if progress_callback:\n                progress_callback(5, \"Memulai deteksi wajah...\")\n                \n            # Process frames\n            processed_frames = 0\n            total_samples = int(duration / self.sample_rate)\n            \n            for timestamp in np.arange(0, duration, self.sample_rate):\n                try:\n                    # Get frame\n                    frame = video.get_frame(timestamp)\n                    \n                    # Detect faces dalam frame\n                    detections = self._detect_faces_in_frame(frame, timestamp)\n                    \n                    # Update tracks\n                    self._update_tracks(detections, timestamp)\n                    \n                    processed_frames += 1\n                    \n                    if progress_callback and processed_frames % 10 == 0:\n                        progress = 5 + (processed_frames / total_samples) * 85\n                        progress_callback(progress, f\"Memproses frame {processed_frames}/{total_samples}...\")\n                        \n                except Exception as e:\n                    logger.warning(f\"Error processing frame at {timestamp}s: {e}\")\n                    continue\n                    \n            # Finalize tracks\n            if progress_callback:\n                progress_callback(95, \"Menganalisis hasil tracking...\")\n                \n            face_analysis = self._analyze_face_tracks(duration)\n            \n            # Cleanup\n            video.close()\n            \n            if progress_callback:\n                progress_callback(100, f\"Face tracking selesai - {len(face_analysis['tracks'])} wajah terdeteksi\")\n                \n            logger.info(f\"Face tracking complete. Detected {len(face_analysis['tracks'])} unique faces\")\n            return face_analysis\n            \n        except Exception as e:\n            logger.error(f\"Error in face tracking: {e}\")\n            return {'tracks': [], 'statistics': {}, 'main_speakers': []}\n            \n    def _detect_faces_in_frame(self, frame, timestamp):\n        \"\"\"\n        Detect faces dalam single frame\n        \"\"\"\n        try:\n            detections = []\n            \n            # Convert BGR to RGB untuk face_recognition\n            rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n            frame_height, frame_width = frame.shape[:2]\n            \n            # Resize frame untuk performance jika terlalu besar\n            scale_factor = 1.0\n            if frame_width > 1280:\n                scale_factor = 1280 / frame_width\n                new_width = int(frame_width * scale_factor)\n                new_height = int(frame_height * scale_factor)\n                rgb_frame = cv2.resize(rgb_frame, (new_width, new_height))\n                \n            # Detect face locations\n            face_locations = face_recognition.face_locations(\n                rgb_frame, \n                model=self.face_detection_model\n            )\n            \n            if not face_locations:\n                return detections\n                \n            # Get face encodings\n            face_encodings = face_recognition.face_encodings(rgb_frame, face_locations)\n            \n            # Process each detected face\n            for i, (face_location, face_encoding) in enumerate(zip(face_locations, face_encodings)):\n                top, right, bottom, left = face_location\n                \n                # Scale back jika frame diresize\n                if scale_factor != 1.0:\n                    top = int(top / scale_factor)\n                    right = int(right / scale_factor)\n                    bottom = int(bottom / scale_factor)\n                    left = int(left / scale_factor)\n                    \n                # Calculate bounding box dan properties\n                width = right - left\n                height = bottom - top\n                face_area = width * height\n                frame_area = frame_width * frame_height\n                relative_size = face_area / frame_area\n                \n                # Filter out faces yang terlalu kecil\n                if relative_size < self.min_face_size:\n                    continue\n                    \n                # Calculate face center\n                center_x = left + width // 2\n                center_y = top + height // 2\n                \n                # Estimate confidence berdasarkan size dan position\n                confidence = min(relative_size * 10, 1.0)  # Simple heuristic\n                \n                if confidence < self.confidence_threshold:\n                    continue\n                    \n                # Get facial landmarks (simplified)\n                landmarks = []\n                try:\n                    face_landmarks_list = face_recognition.face_landmarks(rgb_frame, [face_location])\n                    if face_landmarks_list:\n                        # Extract key points\n                        landmarks_dict = face_landmarks_list[0]\n                        for feature_points in landmarks_dict.values():\n                            landmarks.extend(feature_points)\n                except:\n                    landmarks = None\n                    \n                # Create detection object\n                detection = FaceDetection(\n                    timestamp=timestamp,\n                    face_id=-1,  # Will be assigned during tracking\n                    confidence=confidence,\n                    bounding_box=(left, top, width, height),\n                    landmarks=landmarks,\n                    encoding=face_encoding,\n                    size=relative_size,\n                    center=(center_x, center_y)\n                )\n                \n                detections.append(detection)\n                \n            return detections\n            \n        except Exception as e:\n            logger.error(f\"Error detecting faces in frame: {e}\")\n            return []\n            \n    def _update_tracks(self, detections, timestamp):\n        \"\"\"\n        Update face tracks dengan detections baru\n        \"\"\"\n        try:\n            if not detections:\n                return\n                \n            # Match detections dengan existing tracks\n            matched_tracks = set()\n            \n            for detection in detections:\n                best_match_id = None\n                best_distance = float('inf')\n                \n                # Compare dengan existing tracks\n                for track_id, track in self.face_tracks.items():\n                    if timestamp - track.last_seen > self.track_timeout:\n                        continue  # Track expired\n                        \n                    # Calculate distance menggunakan face encoding\n                    distance = face_recognition.face_distance(\n                        [track.face_encoding], \n                        detection.encoding\n                    )[0]\n                    \n                    if distance < self.max_face_distance and distance < best_distance:\n                        best_distance = distance\n                        best_match_id = track_id\n                        \n                # Assign track ID\n                if best_match_id is not None:\n                    # Update existing track\n                    detection.face_id = best_match_id\n                    self._update_existing_track(best_match_id, detection)\n                    matched_tracks.add(best_match_id)\n                else:\n                    # Create new track\n                    detection.face_id = self.next_face_id\n                    self._create_new_track(detection)\n                    matched_tracks.add(self.next_face_id)\n                    self.next_face_id += 1\n                    \n            # Check untuk tracks yang expired\n            expired_tracks = []\n            for track_id, track in self.face_tracks.items():\n                if timestamp - track.last_seen > self.track_timeout:\n                    expired_tracks.append(track_id)\n                    \n            # Remove expired tracks\n            for track_id in expired_tracks:\n                del self.face_tracks[track_id]\n                \n        except Exception as e:\n            logger.error(f\"Error updating tracks: {e}\")\n            \n    def _create_new_track(self, detection):\n        \"\"\"\n        Create new face track\n        \"\"\"\n        track = FaceTrack(\n            face_id=detection.face_id,\n            first_seen=detection.timestamp,\n            last_seen=detection.timestamp,\n            total_duration=0.0,\n            appearances=1,\n            average_size=detection.size,\n            average_confidence=detection.confidence,\n            face_encoding=detection.encoding.copy(),\n            track_history=[detection]\n        )\n        \n        self.face_tracks[detection.face_id] = track\n        \n    def _update_existing_track(self, track_id, detection):\n        \"\"\"\n        Update existing face track dengan detection baru\n        \"\"\"\n        track = self.face_tracks[track_id]\n        \n        # Update statistics\n        track.last_seen = detection.timestamp\n        track.total_duration = track.last_seen - track.first_seen\n        track.appearances += 1\n        \n        # Update averages\n        track.average_size = ((track.average_size * (track.appearances - 1)) + detection.size) / track.appearances\n        track.average_confidence = ((track.average_confidence * (track.appearances - 1)) + detection.confidence) / track.appearances\n        \n        # Update face encoding (weighted average)\n        alpha = 0.1  # Learning rate\n        track.face_encoding = (1 - alpha) * track.face_encoding + alpha * detection.encoding\n        \n        # Add to history\n        track.track_history.append(detection)\n        \n        # Limit history size untuk memory efficiency\n        if len(track.track_history) > 100:\n            track.track_history = track.track_history[-50:]  # Keep last 50\n            \n    def _analyze_face_tracks(self, total_duration):\n        \"\"\"\n        Analyze face tracks untuk mendapatkan insights\n        \"\"\"\n        try:\n            # Convert tracks ke format yang bisa di-serialize\n            tracks_data = []\n            \n            for track in self.face_tracks.values():\n                # Calculate screen time percentage\n                screen_time_percentage = (track.total_duration / total_duration) * 100\n                \n                # Determine jika ini main speaker berdasarkan screen time dan size\n                is_prominent = (\n                    screen_time_percentage > 10 and  # At least 10% screen time\n                    track.average_size > 0.05 and    # Reasonable size\n                    track.average_confidence > 0.6    # Good confidence\n                )\n                \n                track_data = {\n                    'face_id': track.face_id,\n                    'first_seen': track.first_seen,\n                    'last_seen': track.last_seen,\n                    'total_duration': track.total_duration,\n                    'screen_time_percentage': screen_time_percentage,\n                    'appearances': track.appearances,\n                    'average_size': track.average_size,\n                    'average_confidence': track.average_confidence,\n                    'is_prominent': is_prominent,\n                    'face_encoding': track.face_encoding.tolist(),  # For JSON serialization\n                    'timeline': []\n                }\n                \n                # Sample timeline untuk visualization\n                for i in range(0, len(track.track_history), max(1, len(track.track_history) // 20)):\n                    detection = track.track_history[i]\n                    timeline_point = {\n                        'timestamp': detection.timestamp,\n                        'confidence': detection.confidence,\n                        'size': detection.size,\n                        'center': detection.center,\n                        'bounding_box': detection.bounding_box\n                    }\n                    track_data['timeline'].append(timeline_point)\n                    \n                tracks_data.append(track_data)\n                \n            # Sort tracks by prominence\n            tracks_data.sort(key=lambda x: (x['is_prominent'], x['screen_time_percentage']), reverse=True)\n            \n            # Identify main speakers\n            main_speakers = [track for track in tracks_data if track['is_prominent']]\n            \n            # Calculate statistics\n            statistics = {\n                'total_faces_detected': len(tracks_data),\n                'main_speakers_count': len(main_speakers),\n                'average_faces_per_frame': sum(track['appearances'] for track in tracks_data) / (total_duration / self.sample_rate) if total_duration > 0 else 0,\n                'total_face_time': sum(track['total_duration'] for track in tracks_data),\n                'face_coverage_percentage': (sum(track['total_duration'] for track in tracks_data) / total_duration) * 100 if total_duration > 0 else 0\n            }\n            \n            return {\n                'tracks': tracks_data,\n                'main_speakers': main_speakers,\n                'statistics': statistics,\n                'total_duration': total_duration\n            }\n            \n        except Exception as e:\n            logger.error(f\"Error analyzing face tracks: {e}\")\n            return {'tracks': [], 'main_speakers': [], 'statistics': {}}\n            \n    def register_known_face(self, face_image_path, person_name):\n        \"\"\"\n        Register known face untuk identification\n        \n        Args:\n            face_image_path: Path ke foto wajah\n            person_name: Nama orang\n        \"\"\"\n        try:\n            # Load image\n            image = face_recognition.load_image_file(face_image_path)\n            \n            # Get face encoding\n            encodings = face_recognition.face_encodings(image)\n            \n            if len(encodings) > 0:\n                self.known_faces[person_name] = encodings[0]\n                logger.info(f\"Registered face for {person_name}\")\n                return True\n            else:\n                logger.warning(f\"No face found in image {face_image_path}\")\n                return False\n                \n        except Exception as e:\n            logger.error(f\"Error registering face: {e}\")\n            return False\n            \n    def identify_faces_in_tracks(self, tracks_data):\n        \"\"\"\n        Identify known faces dalam tracking results\n        \"\"\"\n        try:\n            if not self.known_faces:\n                return tracks_data\n                \n            for track in tracks_data['tracks']:\n                track_encoding = np.array(track['face_encoding'])\n                \n                # Compare dengan known faces\n                best_match = None\n                best_distance = float('inf')\n                \n                for person_name, known_encoding in self.known_faces.items():\n                    distance = face_recognition.face_distance([known_encoding], track_encoding)[0]\n                    \n                    if distance < self.face_recognition_tolerance and distance < best_distance:\n                        best_distance = distance\n                        best_match = person_name\n                        \n                # Add identification result\n                if best_match:\n                    track['identified_as'] = best_match\n                    track['identification_confidence'] = 1.0 - best_distance\n                else:\n                    track['identified_as'] = None\n                    track['identification_confidence'] = 0.0\n                    \n            return tracks_data\n            \n        except Exception as e:\n            logger.error(f\"Error identifying faces: {e}\")\n            return tracks_data\n            \n    def get_face_crop_coordinates(self, track_id, video_width, video_height, padding_ratio=0.2):\n        \"\"\"\n        Get koordinat untuk crop wajah dengan padding\n        Useful untuk podcast mode splitting\n        \"\"\"\n        try:\n            if track_id not in self.face_tracks:\n                return None\n                \n            track = self.face_tracks[track_id]\n            \n            # Calculate average position dan size\n            avg_x = np.mean([det.center[0] for det in track.track_history])\n            avg_y = np.mean([det.center[1] for det in track.track_history])\n            avg_width = np.mean([det.bounding_box[2] for det in track.track_history])\n            avg_height = np.mean([det.bounding_box[3] for det in track.track_history])\n            \n            # Add padding\n            padding_x = int(avg_width * padding_ratio)\n            padding_y = int(avg_height * padding_ratio)\n            \n            # Calculate crop coordinates\n            crop_x1 = max(0, int(avg_x - avg_width/2 - padding_x))\n            crop_y1 = max(0, int(avg_y - avg_height/2 - padding_y))\n            crop_x2 = min(video_width, int(avg_x + avg_width/2 + padding_x))\n            crop_y2 = min(video_height, int(avg_y + avg_height/2 + padding_y))\n            \n            return (crop_x1, crop_y1, crop_x2, crop_y2)\n            \n        except Exception as e:\n            logger.error(f\"Error getting crop coordinates: {e}\")\n            return None\n            \n    def save_tracking_results(self, results, output_path):\n        \"\"\"\n        Save tracking results ke file\n        \"\"\"\n        try:\n            with open(output_path, 'w', encoding='utf-8') as f:\n                json.dump(results, f, indent=2, ensure_ascii=False)\n                \n            logger.info(f\"Tracking results saved to {output_path}\")\n            \n        except Exception as e:\n            logger.error(f\"Error saving tracking results: {e}\")\n\n# Test function\nif __name__ == \"__main__\":\n    # Test face tracker\n    tracker = FaceTracker()\n    \n    def test_progress(progress, message):\n        print(f\"Progress: {progress}% - {message}\")\n    \n    print(\"Face Tracker module loaded successfully\")\n    \n    # Test dengan sample video (uncomment untuk testing)\n    # video_path = \"test_video.mp4\"\n    # results = tracker.track_faces(video_path, test_progress)\n    # \n    # print(f\"Detected {len(results['tracks'])} faces\")\n    # for i, track in enumerate(results['tracks']):\n    #     print(f\"Face {i+1}: {track['screen_time_percentage']:.1f}% screen time\")