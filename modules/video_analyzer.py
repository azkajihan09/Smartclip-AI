#!/usr/bin/env python3\n\"\"\"\nVideo Analyzer Module\nMenganalisis video untuk mendeteksi moment terbaik menggunakan AI\nFitur: Scene detection, audio analysis, visual engagement, content analysis\n\"\"\"\n\nimport cv2\nimport numpy as np\nimport torch\nimport librosa\nimport logging\nfrom pathlib import Path\nfrom typing import List, Dict, Tuple\nimport json\nfrom dataclasses import dataclass\nfrom moviepy.editor import VideoFileClip\nimport matplotlib.pyplot as plt\nfrom scipy import signal\nfrom sklearn.cluster import KMeans\nfrom transformers import pipeline\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# Setup logging\nlogging.basicConfig(level=logging.INFO)\nlogger = logging.getLogger(__name__)\n\n@dataclass\nclass VideoMoment:\n    \"\"\"Data class untuk menyimpan informasi moment video\"\"\"\n    start_time: float\n    end_time: float\n    duration: float\n    score: float\n    reason: str\n    features: Dict\n    confidence: float\n    \nclass VideoAnalyzer:\n    def __init__(self, models_dir=None):\n        \"\"\"Initialize video analyzer\"\"\"\n        self.models_dir = Path(models_dir) if models_dir else Path(__file__).parent.parent / \"models\"\n        self.models_dir.mkdir(exist_ok=True)\n        \n        # Initialize AI models\n        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n        logger.info(f\"Using device: {self.device}\")\n        \n        # Load pre-trained models\n        self._load_models()\n        \n        # Analysis parameters\n        self.window_size = 5.0  # seconds\n        self.step_size = 1.0    # seconds\n        self.min_moment_duration = 5.0\n        self.max_moment_duration = 60.0\n        \n    def _load_models(self):\n        \"\"\"Load AI models untuk analysis\"\"\"\n        try:\n            # Audio classification untuk mood detection\n            logger.info(\"Loading audio analysis models...\")\n            \n            # Emotion detection dari audio (jika available)\n            try:\n                self.emotion_classifier = pipeline(\n                    \"audio-classification\",\n                    model=\"ehcalabres/wav2vec2-lg-xlsr-en-speech-emotion-recognition\",\n                    device=0 if torch.cuda.is_available() else -1\n                )\n                logger.info(\"Audio emotion model loaded\")\n            except Exception as e:\n                logger.warning(f\"Could not load emotion model: {e}\")\n                self.emotion_classifier = None\n                \n            # Visual scene analysis\n            logger.info(\"Loading visual analysis models...\")\n            \n            # Object detection untuk content analysis\n            try:\n                from ultralytics import YOLO\n                self.object_detector = YOLO('yolov8n.pt')  # Lightweight model\n                logger.info(\"Object detection model loaded\")\n            except Exception as e:\n                logger.warning(f\"Could not load object detection: {e}\")\n                self.object_detector = None\n                \n            logger.info(\"Models loaded successfully\")\n            \n        except Exception as e:\n            logger.error(f\"Error loading models: {e}\")\n            \n    def analyze_video(self, video_path, progress_callback=None):\n        \"\"\"\n        Main function untuk menganalisis video dan menemukan moment terbaik\n        \n        Args:\n            video_path: Path ke file video\n            progress_callback: Function untuk update progress\n            \n        Returns:\n            List of VideoMoment objects\n        \"\"\"\n        try:\n            logger.info(f\"Starting video analysis: {video_path}\")\n            \n            # Load video\n            video = VideoFileClip(video_path)\n            duration = video.duration\n            fps = video.fps\n            \n            if progress_callback:\n                progress_callback(5, \"Menganalisis struktur video...\")\n                \n            # Step 1: Scene detection\n            scenes = self._detect_scenes(video, progress_callback)\n            \n            if progress_callback:\n                progress_callback(25, \"Menganalisis audio...\")\n                \n            # Step 2: Audio analysis\n            audio_features = self._analyze_audio(video, progress_callback)\n            \n            if progress_callback:\n                progress_callback(50, \"Menganalisis visual content...\")\n                \n            # Step 3: Visual analysis\n            visual_features = self._analyze_visual_content(video, progress_callback)\n            \n            if progress_callback:\n                progress_callback(75, \"Menghitung moment scores...\")\n                \n            # Step 4: Combine features dan score moments\n            moments = self._score_moments(scenes, audio_features, visual_features, duration)\n            \n            if progress_callback:\n                progress_callback(90, \"Memfilter moment terbaik...\")\n                \n            # Step 5: Filter dan rank moments\n            best_moments = self._filter_and_rank_moments(moments)\n            \n            # Cleanup\n            video.close()\n            \n            if progress_callback:\n                progress_callback(100, f\"Analisis selesai - {len(best_moments)} moment terdeteksi\")\n                \n            logger.info(f\"Analysis complete. Found {len(best_moments)} best moments\")\n            return best_moments\n            \n        except Exception as e:\n            logger.error(f\"Error analyzing video: {e}\")\n            return []\n            \n    def _detect_scenes(self, video, progress_callback=None):\n        \"\"\"\n        Detect scene changes dalam video\n        \"\"\"\n        try:\n            scenes = []\n            duration = video.duration\n            \n            # Sample frames untuk scene detection\n            sample_interval = 1.0  # Every second\n            frames = []\n            timestamps = []\n            \n            for t in np.arange(0, duration, sample_interval):\n                try:\n                    frame = video.get_frame(t)\n                    frames.append(frame)\n                    timestamps.append(t)\n                except:\n                    continue\n                    \n            if len(frames) < 2:\n                return [(0, duration)]\n                \n            # Calculate frame differences\n            scene_changes = [0]  # Start dengan frame pertama\n            \n            for i in range(1, len(frames)):\n                # Convert ke grayscale dan resize untuk efficiency\n                frame1 = cv2.cvtColor(frames[i-1], cv2.COLOR_RGB2GRAY)\n                frame2 = cv2.cvtColor(frames[i], cv2.COLOR_RGB2GRAY)\n                \n                frame1 = cv2.resize(frame1, (64, 48))\n                frame2 = cv2.resize(frame2, (64, 48))\n                \n                # Calculate histogram difference\n                diff = cv2.compareHist(\n                    cv2.calcHist([frame1], [0], None, [256], [0, 256]),\n                    cv2.calcHist([frame2], [0], None, [256], [0, 256]),\n                    cv2.HISTCMP_CORREL\n                )\n                \n                # Threshold untuk scene change (semakin rendah = scene change)\n                if diff < 0.8:  # Tunable parameter\n                    scene_changes.append(timestamps[i])\n                    \n            scene_changes.append(duration)  # End dengan frame terakhir\n            \n            # Convert ke scene segments\n            for i in range(len(scene_changes) - 1):\n                start_time = scene_changes[i]\n                end_time = scene_changes[i + 1]\n                scenes.append((start_time, end_time))\n                \n            logger.info(f\"Detected {len(scenes)} scenes\")\n            return scenes\n            \n        except Exception as e:\n            logger.error(f\"Error detecting scenes: {e}\")\n            return [(0, video.duration)]  # Fallback: whole video as one scene\n            \n    def _analyze_audio(self, video, progress_callback=None):\n        \"\"\"\n        Analyze audio features untuk menentukan engagement\n        \"\"\"\n        try:\n            # Extract audio\n            audio = video.audio\n            if not audio:\n                return {'energy': [], 'tempo': [], 'spectral_features': [], 'emotions': []}\n                \n            # Get audio array\n            audio_array = audio.to_soundarray()\n            if len(audio_array.shape) > 1:\n                audio_array = np.mean(audio_array, axis=1)  # Convert to mono\n                \n            sample_rate = audio.fps\n            duration = len(audio_array) / sample_rate\n            \n            # Calculate audio features in windows\n            window_length = int(self.window_size * sample_rate)\n            step_length = int(self.step_size * sample_rate)\n            \n            features = {\n                'energy': [],\n                'tempo': [],\n                'spectral_features': [],\n                'emotions': [],\n                'timestamps': []\n            }\n            \n            for start in range(0, len(audio_array) - window_length, step_length):\n                window = audio_array[start:start + window_length]\n                timestamp = start / sample_rate\n                \n                # Energy (RMS)\n                energy = np.sqrt(np.mean(window ** 2))\n                \n                # Tempo estimation\n                try:\n                    tempo, _ = librosa.beat.beat_track(y=window, sr=sample_rate)\n                    tempo = float(tempo) if not np.isnan(tempo) else 120.0\n                except:\n                    tempo = 120.0\n                    \n                # Spectral features\n                spectral_centroid = np.mean(librosa.feature.spectral_centroid(y=window, sr=sample_rate))\n                spectral_rolloff = np.mean(librosa.feature.spectral_rolloff(y=window, sr=sample_rate))\n                zero_crossing_rate = np.mean(librosa.feature.zero_crossing_rate(window))\n                \n                # Emotion detection (if model available)\n                emotion_score = 0.5  # Default neutral\n                if self.emotion_classifier and len(window) > 1024:\n                    try:\n                        # Resample untuk model jika perlu\n                        if sample_rate != 16000:\n                            window_resampled = librosa.resample(window, orig_sr=sample_rate, target_sr=16000)\n                        else:\n                            window_resampled = window\n                            \n                        emotion_result = self.emotion_classifier(window_resampled)\n                        # Extract positive emotion score\n                        emotion_score = max([r['score'] for r in emotion_result if r['label'] in ['happy', 'excited', 'positive']], default=0.5)\n                    except:\n                        emotion_score = 0.5\n                        \n                features['energy'].append(energy)\n                features['tempo'].append(tempo)\n                features['spectral_features'].append({\n                    'centroid': float(spectral_centroid),\n                    'rolloff': float(spectral_rolloff),\n                    'zcr': float(zero_crossing_rate)\n                })\n                features['emotions'].append(emotion_score)\n                features['timestamps'].append(timestamp)\n                \n            return features\n            \n        except Exception as e:\n            logger.error(f\"Error analyzing audio: {e}\")\n            return {'energy': [], 'tempo': [], 'spectral_features': [], 'emotions': []}\n            \n    def _analyze_visual_content(self, video, progress_callback=None):\n        \"\"\"\n        Analyze visual content untuk engagement scoring\n        \"\"\"\n        try:\n            duration = video.duration\n            features = {\n                'motion': [],\n                'objects': [],\n                'face_count': [],\n                'color_variance': [],\n                'brightness': [],\n                'timestamps': []\n            }\n            \n            # Sample frames\n            sample_interval = self.step_size\n            \n            prev_frame = None\n            for t in np.arange(0, duration, sample_interval):\n                try:\n                    frame = video.get_frame(t)\n                    gray_frame = cv2.cvtColor(frame, cv2.COLOR_RGB2GRAY)\n                    \n                    # Motion detection\n                    motion_score = 0.0\n                    if prev_frame is not None:\n                        diff = cv2.absdiff(prev_frame, gray_frame)\n                        motion_score = np.mean(diff) / 255.0\n                        \n                    prev_frame = gray_frame\n                    \n                    # Object detection\n                    object_count = 0\n                    object_confidence = 0.0\n                    if self.object_detector:\n                        try:\n                            results = self.object_detector(frame, verbose=False)\n                            if len(results) > 0 and len(results[0].boxes) > 0:\n                                object_count = len(results[0].boxes)\n                                object_confidence = float(np.mean([box.conf.cpu().numpy() for box in results[0].boxes]))\n                        except:\n                            pass\n                            \n                    # Face detection (simple)\n                    face_count = 0\n                    try:\n                        face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')\n                        faces = face_cascade.detectMultiScale(gray_frame, 1.3, 5)\n                        face_count = len(faces)\n                    except:\n                        pass\n                        \n                    # Color variance (diversity)\n                    color_var = np.std(frame.reshape(-1, 3), axis=0).mean()\n                    \n                    # Brightness\n                    brightness = np.mean(gray_frame) / 255.0\n                    \n                    features['motion'].append(motion_score)\n                    features['objects'].append({'count': object_count, 'confidence': object_confidence})\n                    features['face_count'].append(face_count)\n                    features['color_variance'].append(color_var)\n                    features['brightness'].append(brightness)\n                    features['timestamps'].append(t)\n                    \n                except Exception as e:\n                    logger.warning(f\"Error processing frame at {t}s: {e}\")\n                    continue\n                    \n            return features\n            \n        except Exception as e:\n            logger.error(f\"Error analyzing visual content: {e}\")\n            return {'motion': [], 'objects': [], 'face_count': [], 'color_variance': [], 'brightness': [], 'timestamps': []}\n            \n    def _score_moments(self, scenes, audio_features, visual_features, duration):\n        \"\"\"\n        Score moments berdasarkan combined features\n        \"\"\"\n        try:\n            moments = []\n            \n            # Normalize features untuk scoring\n            audio_energy = np.array(audio_features.get('energy', [0]))\n            audio_emotions = np.array(audio_features.get('emotions', [0.5]))\n            visual_motion = np.array(visual_features.get('motion', [0]))\n            face_counts = np.array(visual_features.get('face_count', [0]))\n            \n            # Normalize arrays\n            if len(audio_energy) > 0:\n                audio_energy = (audio_energy - np.min(audio_energy)) / (np.max(audio_energy) - np.min(audio_energy) + 1e-8)\n            if len(visual_motion) > 0:\n                visual_motion = (visual_motion - np.min(visual_motion)) / (np.max(visual_motion) - np.min(visual_motion) + 1e-8)\n                \n            # Score each scene\n            for start_time, end_time in scenes:\n                scene_duration = end_time - start_time\n                \n                if scene_duration < self.min_moment_duration:\n                    continue\n                    \n                # Find features dalam time range\n                audio_timestamps = audio_features.get('timestamps', [])\n                visual_timestamps = visual_features.get('timestamps', [])\n                \n                # Audio features untuk scene\n                audio_indices = [i for i, t in enumerate(audio_timestamps) if start_time <= t <= end_time]\n                visual_indices = [i for i, t in enumerate(visual_timestamps) if start_time <= t <= end_time]\n                \n                if not audio_indices and not visual_indices:\n                    continue\n                    \n                # Calculate scores\n                audio_score = 0.0\n                if audio_indices:\n                    scene_energy = np.mean([audio_energy[i] for i in audio_indices] if len(audio_energy) > 0 else [0])\n                    scene_emotion = np.mean([audio_emotions[i] for i in audio_indices] if len(audio_emotions) > 0 else [0.5])\n                    audio_score = 0.6 * scene_energy + 0.4 * scene_emotion\n                    \n                visual_score = 0.0\n                if visual_indices:\n                    scene_motion = np.mean([visual_motion[i] for i in visual_indices] if len(visual_motion) > 0 else [0])\n                    scene_faces = np.mean([face_counts[i] for i in visual_indices] if len(face_counts) > 0 else [0])\n                    visual_score = 0.7 * scene_motion + 0.3 * min(scene_faces / 3.0, 1.0)  # Normalize face count\n                    \n                # Combined score\n                combined_score = 0.4 * audio_score + 0.6 * visual_score\n                \n                # Boost score untuk optimal duration\n                duration_factor = 1.0\n                if 15 <= scene_duration <= 45:  # Optimal range\n                    duration_factor = 1.2\n                elif scene_duration > 60:\n                    duration_factor = 0.8\n                    \n                final_score = combined_score * duration_factor\n                \n                # Create moment\n                moment = VideoMoment(\n                    start_time=start_time,\n                    end_time=end_time,\n                    duration=scene_duration,\n                    score=final_score,\n                    reason=self._generate_reason(audio_score, visual_score, scene_duration),\n                    features={\n                        'audio_score': audio_score,\n                        'visual_score': visual_score,\n                        'scene_duration': scene_duration,\n                        'face_count': np.mean([face_counts[i] for i in visual_indices]) if visual_indices and len(face_counts) > 0 else 0\n                    },\n                    confidence=min(final_score, 1.0)\n                )\n                \n                moments.append(moment)\n                \n            return moments\n            \n        except Exception as e:\n            logger.error(f\"Error scoring moments: {e}\")\n            return []\n            \n    def _filter_and_rank_moments(self, moments, max_moments=10):\n        \"\"\"\n        Filter dan rank moments untuk mendapatkan yang terbaik\n        \"\"\"\n        try:\n            if not moments:\n                return []\n                \n            # Sort by score descending\n            moments.sort(key=lambda x: x.score, reverse=True)\n            \n            # Filter overlapping moments (ambil yang score tertinggi)\n            filtered_moments = []\n            \n            for moment in moments:\n                # Check overlap dengan moments yang sudah dipilih\n                overlaps = False\n                for selected_moment in filtered_moments:\n                    if (moment.start_time < selected_moment.end_time and \n                        moment.end_time > selected_moment.start_time):\n                        overlaps = True\n                        break\n                        \n                if not overlaps:\n                    filtered_moments.append(moment)\n                    \n                if len(filtered_moments) >= max_moments:\n                    break\n                    \n            # Additional filtering berdasarkan score threshold\n            threshold = 0.3  # Minimum score\n            final_moments = [m for m in filtered_moments if m.score >= threshold]\n            \n            return final_moments\n            \n        except Exception as e:\n            logger.error(f\"Error filtering moments: {e}\")\n            return moments[:max_moments] if moments else []\n            \n    def _generate_reason(self, audio_score, visual_score, duration):\n        \"\"\"\n        Generate human-readable reason untuk moment selection\n        \"\"\"\n        reasons = []\n        \n        if audio_score > 0.7:\n            reasons.append(\"Audio engaging\")\n        if visual_score > 0.7:\n            reasons.append(\"Visual menarik\")\n        if 15 <= duration <= 45:\n            reasons.append(\"Durasi optimal\")\n        if audio_score > 0.6 and visual_score > 0.6:\n            reasons.append(\"Kombinasi audio-visual bagus\")\n            \n        if not reasons:\n            if audio_score > visual_score:\n                reasons.append(\"Audio cukup menarik\")\n            else:\n                reasons.append(\"Visual cukup menarik\")\n                \n        return \", \".join(reasons)\n        \n    def save_analysis_results(self, moments, output_path):\n        \"\"\"\n        Save analysis results ke file JSON\n        \"\"\"\n        try:\n            results = {\n                'analysis_timestamp': str(pd.Timestamp.now()),\n                'total_moments': len(moments),\n                'moments': []\n            }\n            \n            for moment in moments:\n                moment_data = {\n                    'start_time': moment.start_time,\n                    'end_time': moment.end_time,\n                    'duration': moment.duration,\n                    'score': moment.score,\n                    'reason': moment.reason,\n                    'confidence': moment.confidence,\n                    'features': moment.features\n                }\n                results['moments'].append(moment_data)\n                \n            with open(output_path, 'w', encoding='utf-8') as f:\n                json.dump(results, f, indent=2, ensure_ascii=False)\n                \n            logger.info(f\"Analysis results saved to {output_path}\")\n            \n        except Exception as e:\n            logger.error(f\"Error saving analysis results: {e}\")\n\n# Test function\nif __name__ == \"__main__\":\n    # Test analyzer\n    analyzer = VideoAnalyzer()\n    \n    def test_progress(progress, message):\n        print(f\"Progress: {progress}% - {message}\")\n    \n    # Test dengan sample video (ganti dengan path video actual)\n    # video_path = \"test_video.mp4\"\n    # moments = analyzer.analyze_video(video_path, test_progress)\n    # \n    # for i, moment in enumerate(moments):\n    #     print(f\"Moment {i+1}: {moment.start_time:.1f}s - {moment.end_time:.1f}s\")\n    #     print(f\"  Score: {moment.score:.3f}, Reason: {moment.reason}\")\n    \n    print(\"Video Analyzer module loaded successfully\")