#!/usr/bin/env python3\n\"\"\"\nSpeaker Diarization Module\nIdentifikasi dan tracking siapa yang berbicara kapan dalam video\nMenggunakan AI untuk mengenali suara dan memisahkan pembicara\n\"\"\"\n\nimport torch\nimport torchaudio\nimport numpy as np\nimport librosa\nfrom pathlib import Path\nimport logging\nfrom typing import List, Dict, Tuple, Optional\nfrom dataclasses import dataclass\nimport json\nimport pickle\nfrom moviepy.editor import VideoFileClip\nfrom scipy.spatial.distance import cosine\nfrom sklearn.cluster import AgglomerativeClustering\nfrom collections import defaultdict\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Pyannote.audio untuk speaker diarization\ntry:\n    from pyannote.audio import Pipeline\n    from pyannote.audio.pipelines.utils.hook import ProgressHook\n    PYANNOTE_AVAILABLE = True\nexcept ImportError:\n    PYANNOTE_AVAILABLE = False\n    logging.warning(\"Pyannote.audio not available. Using alternative speaker diarization.\")\n\n# SpeechBrain untuk speaker embeddings\ntry:\n    import speechbrain as sb\n    from speechbrain.pretrained import EncoderClassifier\n    SPEECHBRAIN_AVAILABLE = True\nexcept ImportError:\n    SPEECHBRAIN_AVAILABLE = False\n    logging.warning(\"SpeechBrain not available. Using alternative speaker identification.\")\n\n# Setup logging\nlogging.basicConfig(level=logging.INFO)\nlogger = logging.getLogger(__name__)\n\n@dataclass\nclass SpeechSegment:\n    \"\"\"Data class untuk speech segment\"\"\"\n    start_time: float\n    end_time: float\n    duration: float\n    speaker_id: int\n    confidence: float\n    text: Optional[str] = None\n    embedding: Optional[np.ndarray] = None\n    energy: float = 0.0\n    pitch: float = 0.0\n    \n@dataclass\nclass SpeakerProfile:\n    \"\"\"Data class untuk speaker profile\"\"\"\n    speaker_id: int\n    name: Optional[str]\n    total_duration: float\n    speech_percentage: float\n    average_energy: float\n    average_pitch: float\n    voice_embedding: np.ndarray\n    speech_segments: List[SpeechSegment]\n    characteristics: Dict\n    \nclass SpeakerDiarization:\n    def __init__(self, models_dir=None, use_auth_token=None):\n        \"\"\"Initialize speaker diarization\n        \n        Args:\n            models_dir: Directory untuk menyimpan models\n            use_auth_token: Hugging Face auth token untuk pyannote models\n        \"\"\"\n        self.models_dir = Path(models_dir) if models_dir else Path(__file__).parent.parent / \"models\"\n        self.models_dir.mkdir(exist_ok=True)\n        \n        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n        logger.info(f\"Using device: {self.device}\")\n        \n        # Parameters\n        self.min_speech_duration = 1.0  # Minimum 1 second\n        self.clustering_threshold = 0.7  # For speaker clustering\n        self.voice_activity_threshold = 0.5\n        \n        # Initialize models\n        self.diarization_pipeline = None\n        self.speaker_encoder = None\n        self.use_auth_token = use_auth_token\n        \n        self._load_models()\n        \n    def _load_models(self):\n        \"\"\"Load AI models untuk speaker diarization\"\"\"\n        try:\n            # Load pyannote diarization pipeline\n            if PYANNOTE_AVAILABLE:\n                logger.info(\"Loading pyannote.audio diarization pipeline...\")\n                try:\n                    # Note: Butuh HuggingFace token untuk model ini\n                    self.diarization_pipeline = Pipeline.from_pretrained(\n                        \"pyannote/speaker-diarization-3.1\",\n                        use_auth_token=self.use_auth_token\n                    )\n                    \n                    if torch.cuda.is_available():\n                        self.diarization_pipeline = self.diarization_pipeline.to(torch.device(\"cuda\"))\n                        \n                    logger.info(\"Pyannote diarization pipeline loaded\")\n                except Exception as e:\n                    logger.warning(f\"Could not load pyannote pipeline: {e}\")\n                    logger.warning(\"Will use alternative diarization method\")\n                    \n            # Load speaker embedding model\n            if SPEECHBRAIN_AVAILABLE:\n                logger.info(\"Loading SpeechBrain speaker encoder...\")\n                try:\n                    self.speaker_encoder = EncoderClassifier.from_hparams(\n                        source=\"speechbrain/spkrec-ecapa-voxceleb\",\n                        savedir=str(self.models_dir / \"speaker_encoder\"),\n                        run_opts={\"device\": self.device}\n                    )\n                    logger.info(\"SpeechBrain speaker encoder loaded\")\n                except Exception as e:\n                    logger.warning(f\"Could not load SpeechBrain encoder: {e}\")\n                    \n        except Exception as e:\n            logger.error(f\"Error loading models: {e}\")\n            \n    def identify_speakers(self, video_path, progress_callback=None):\n        \"\"\"\n        Main function untuk speaker diarization\n        \n        Args:\n            video_path: Path ke video file\n            progress_callback: Function untuk progress updates\n            \n        Returns:\n            Dict dengan speaker diarization results\n        \"\"\"\n        try:\n            logger.info(f\"Starting speaker diarization: {video_path}\")\n            \n            if progress_callback:\n                progress_callback(5, \"Mengekstrak audio dari video...\")\n                \n            # Extract audio dari video\n            audio_path = self._extract_audio(video_path)\n            if not audio_path:\n                return self._empty_result()\n                \n            if progress_callback:\n                progress_callback(15, \"Memuat audio untuk analisis...\")\n                \n            # Load audio\n            audio_data, sample_rate = self._load_audio(audio_path)\n            duration = len(audio_data) / sample_rate\n            \n            if progress_callback:\n                progress_callback(25, \"Mendeteksi aktivitas suara...\")\n                \n            # Voice Activity Detection (VAD)\n            voice_segments = self._detect_voice_activity(audio_data, sample_rate)\n            \n            if progress_callback:\n                progress_callback(50, \"Melakukan speaker diarization...\")\n                \n            # Speaker diarization\n            if self.diarization_pipeline:\n                # Use pyannote pipeline\n                diarization_result = self._pyannote_diarization(audio_path)\n            else:\n                # Use alternative method\n                diarization_result = self._alternative_diarization(audio_data, sample_rate, voice_segments)\n                \n            if progress_callback:\n                progress_callback(75, \"Menganalisis karakteristik pembicara...\")\n                \n            # Analyze speaker characteristics\n            speaker_profiles = self._analyze_speakers(audio_data, sample_rate, diarization_result)\n            \n            if progress_callback:\n                progress_callback(90, \"Memproses hasil analisis...\")\n                \n            # Generate final results\n            results = self._generate_results(speaker_profiles, duration)\n            \n            # Cleanup temporary audio file\n            try:\n                Path(audio_path).unlink()\n            except:\n                pass\n                \n            if progress_callback:\n                progress_callback(100, f\"Speaker diarization selesai - {len(speaker_profiles)} pembicara terdeteksi\")\n                \n            logger.info(f\"Speaker diarization complete. Identified {len(speaker_profiles)} speakers\")\n            return results\n            \n        except Exception as e:\n            logger.error(f\"Error in speaker diarization: {e}\")\n            return self._empty_result()\n            \n    def _extract_audio(self, video_path):\n        \"\"\"Extract audio dari video file\"\"\"\n        try:\n            video = VideoFileClip(video_path)\n            audio = video.audio\n            \n            if not audio:\n                logger.warning(\"No audio track found in video\")\n                return None\n                \n            # Save audio ke temporary file\n            audio_path = self.models_dir / \"temp_audio.wav\"\n            audio.write_audiofile(str(audio_path), verbose=False, logger=None)\n            \n            # Cleanup\n            audio.close()\n            video.close()\n            \n            return str(audio_path)\n            \n        except Exception as e:\n            logger.error(f\"Error extracting audio: {e}\")\n            return None\n            \n    def _load_audio(self, audio_path):\n        \"\"\"Load audio file\"\"\"\n        try:\n            # Load dengan librosa untuk consistency\n            audio_data, sample_rate = librosa.load(audio_path, sr=16000)  # 16kHz untuk most models\n            return audio_data, sample_rate\n            \n        except Exception as e:\n            logger.error(f\"Error loading audio: {e}\")\n            return np.array([]), 16000\n            \n    def _detect_voice_activity(self, audio_data, sample_rate):\n        \"\"\"Detect voice activity dalam audio\"\"\"\n        try:\n            # Simple VAD menggunakan energy threshold\n            frame_length = int(0.025 * sample_rate)  # 25ms frames\n            hop_length = int(0.010 * sample_rate)    # 10ms hop\n            \n            # Calculate energy\n            energy = librosa.feature.rms(y=audio_data, frame_length=frame_length, hop_length=hop_length)[0]\n            \n            # Threshold untuk voice activity\n            energy_threshold = np.percentile(energy, 30)  # Dynamic threshold\n            \n            # Find voice segments\n            voice_frames = energy > energy_threshold\n            \n            # Convert frame indices ke time segments\n            segments = []\n            in_segment = False\n            segment_start = 0\n            \n            for i, is_voice in enumerate(voice_frames):\n                time = i * hop_length / sample_rate\n                \n                if is_voice and not in_segment:\n                    segment_start = time\n                    in_segment = True\n                elif not is_voice and in_segment:\n                    if time - segment_start >= self.min_speech_duration:\n                        segments.append((segment_start, time))\n                    in_segment = False\n                    \n            # Handle last segment\n            if in_segment:\n                final_time = len(audio_data) / sample_rate\n                if final_time - segment_start >= self.min_speech_duration:\n                    segments.append((segment_start, final_time))\n                    \n            logger.info(f\"Detected {len(segments)} voice segments\")\n            return segments\n            \n        except Exception as e:\n            logger.error(f\"Error in voice activity detection: {e}\")\n            return []\n            \n    def _pyannote_diarization(self, audio_path):\n        \"\"\"Use pyannote.audio untuk speaker diarization\"\"\"\n        try:\n            if not self.diarization_pipeline:\n                return []\n                \n            # Apply diarization\n            diarization = self.diarization_pipeline(audio_path)\n            \n            # Convert ke format yang kita butuhkan\n            segments = []\n            for turn, _, speaker in diarization.itertracks(yield_label=True):\n                segment = SpeechSegment(\n                    start_time=turn.start,\n                    end_time=turn.end,\n                    duration=turn.duration,\n                    speaker_id=int(speaker.split('_')[-1]) if '_' in speaker else hash(speaker) % 1000,\n                    confidence=1.0  # Pyannote doesn't provide confidence scores\n                )\n                segments.append(segment)\n                \n            return segments\n            \n        except Exception as e:\n            logger.error(f\"Error in pyannote diarization: {e}\")\n            return []\n            \n    def _alternative_diarization(self, audio_data, sample_rate, voice_segments):\n        \"\"\"Alternative speaker diarization using clustering\"\"\"\n        try:\n            if not voice_segments:\n                return []\n                \n            # Extract speaker embeddings untuk setiap voice segment\n            embeddings = []\n            valid_segments = []\n            \n            for start_time, end_time in voice_segments:\n                start_sample = int(start_time * sample_rate)\n                end_sample = int(end_time * sample_rate)\n                \n                segment_audio = audio_data[start_sample:end_sample]\n                \n                if len(segment_audio) < sample_rate * 0.5:  # Skip segments < 0.5s\n                    continue\n                    \n                # Get speaker embedding\n                embedding = self._get_speaker_embedding(segment_audio, sample_rate)\n                \n                if embedding is not None:\n                    embeddings.append(embedding)\n                    valid_segments.append((start_time, end_time))\n                    \n            if len(embeddings) < 2:\n                # Not enough segments for clustering\n                segments = []\n                for i, (start_time, end_time) in enumerate(valid_segments):\n                    segment = SpeechSegment(\n                        start_time=start_time,\n                        end_time=end_time,\n                        duration=end_time - start_time,\n                        speaker_id=0,\n                        confidence=0.8,\n                        embedding=embeddings[i] if i < len(embeddings) else None\n                    )\n                    segments.append(segment)\n                return segments\n                \n            # Cluster embeddings untuk identify speakers\n            embeddings_array = np.vstack(embeddings)\n            \n            # Use agglomerative clustering\n            n_speakers = min(len(embeddings), 5)  # Max 5 speakers\n            clustering = AgglomerativeClustering(\n                n_clusters=None,\n                distance_threshold=self.clustering_threshold,\n                linkage='average'\n            )\n            \n            speaker_labels = clustering.fit_predict(embeddings_array)\n            \n            # Create segments dengan speaker labels\n            segments = []\n            for i, (start_time, end_time) in enumerate(valid_segments):\n                segment = SpeechSegment(\n                    start_time=start_time,\n                    end_time=end_time,\n                    duration=end_time - start_time,\n                    speaker_id=int(speaker_labels[i]),\n                    confidence=0.8,  # Default confidence\n                    embedding=embeddings[i]\n                )\n                segments.append(segment)\n                \n            logger.info(f\"Identified {len(set(speaker_labels))} speakers using clustering\")\n            return segments\n            \n        except Exception as e:\n            logger.error(f\"Error in alternative diarization: {e}\")\n            return []\n            \n    def _get_speaker_embedding(self, audio_segment, sample_rate):\n        \"\"\"Get speaker embedding untuk audio segment\"\"\"\n        try:\n            if self.speaker_encoder:\n                # Use SpeechBrain encoder\n                # Convert ke tensor\n                audio_tensor = torch.FloatTensor(audio_segment).unsqueeze(0)\n                \n                # Get embedding\n                with torch.no_grad():\n                    embedding = self.speaker_encoder.encode_batch(audio_tensor)\n                    return embedding.squeeze().cpu().numpy()\n            else:\n                # Use simple MFCC features sebagai fallback\n                mfccs = librosa.feature.mfcc(y=audio_segment, sr=sample_rate, n_mfcc=13)\n                return np.mean(mfccs, axis=1)\n                \n        except Exception as e:\n            logger.warning(f\"Error getting speaker embedding: {e}\")\n            return None\n            \n    def _analyze_speakers(self, audio_data, sample_rate, speech_segments):\n        \"\"\"Analyze speaker characteristics\"\"\"\n        try:\n            # Group segments by speaker\n            speaker_segments = defaultdict(list)\n            for segment in speech_segments:\n                speaker_segments[segment.speaker_id].append(segment)\n                \n            speaker_profiles = []\n            \n            for speaker_id, segments in speaker_segments.items():\n                # Calculate statistics\n                total_duration = sum(seg.duration for seg in segments)\n                \n                # Analyze audio characteristics untuk speaker\n                speaker_audio_segments = []\n                energies = []\n                pitches = []\n                \n                for segment in segments:\n                    start_sample = int(segment.start_time * sample_rate)\n                    end_sample = int(segment.end_time * sample_rate)\n                    seg_audio = audio_data[start_sample:end_sample]\n                    \n                    if len(seg_audio) > 0:\n                        speaker_audio_segments.append(seg_audio)\n                        \n                        # Energy\n                        energy = np.sqrt(np.mean(seg_audio ** 2))\n                        energies.append(energy)\n                        \n                        # Pitch\n                        try:\n                            pitches_hz = librosa.yin(seg_audio, fmin=50, fmax=400, sr=sample_rate)\n                            valid_pitches = pitches_hz[pitches_hz > 0]\n                            if len(valid_pitches) > 0:\n                                pitches.append(np.median(valid_pitches))\n                        except:\n                            pass\n                            \n                # Create combined embedding untuk speaker\n                if speaker_audio_segments:\n                    combined_audio = np.concatenate(speaker_audio_segments)\n                    voice_embedding = self._get_speaker_embedding(combined_audio, sample_rate)\n                else:\n                    voice_embedding = np.zeros(13)  # Default size\n                    \n                # Speaker characteristics\n                characteristics = {\n                    'average_segment_duration': total_duration / len(segments),\n                    'speech_rate': len(segments) / (segments[-1].end_time - segments[0].start_time) if len(segments) > 1 else 0,\n                    'energy_variance': np.var(energies) if energies else 0,\n                    'pitch_range': np.ptp(pitches) if pitches else 0\n                }\n                \n                profile = SpeakerProfile(\n                    speaker_id=speaker_id,\n                    name=f\"Speaker {speaker_id + 1}\",\n                    total_duration=total_duration,\n                    speech_percentage=0,  # Will be calculated later\n                    average_energy=np.mean(energies) if energies else 0,\n                    average_pitch=np.mean(pitches) if pitches else 0,\n                    voice_embedding=voice_embedding if voice_embedding is not None else np.zeros(13),\n                    speech_segments=segments,\n                    characteristics=characteristics\n                )\n                \n                speaker_profiles.append(profile)\n                \n            return speaker_profiles\n            \n        except Exception as e:\n            logger.error(f\"Error analyzing speakers: {e}\")\n            return []\n            \n    def _generate_results(self, speaker_profiles, total_duration):\n        \"\"\"Generate final results\"\"\"\n        try:\n            # Calculate speech percentages\n            total_speech_time = sum(profile.total_duration for profile in speaker_profiles)\n            \n            for profile in speaker_profiles:\n                if total_speech_time > 0:\n                    profile.speech_percentage = (profile.total_duration / total_speech_time) * 100\n                    \n            # Sort by speech time\n            speaker_profiles.sort(key=lambda x: x.total_duration, reverse=True)\n            \n            # Convert ke format serializable\n            speakers_data = []\n            for profile in speaker_profiles:\n                speaker_data = {\n                    'speaker_id': profile.speaker_id,\n                    'name': profile.name,\n                    'total_duration': profile.total_duration,\n                    'speech_percentage': profile.speech_percentage,\n                    'average_energy': float(profile.average_energy),\n                    'average_pitch': float(profile.average_pitch),\n                    'voice_embedding': profile.voice_embedding.tolist(),\n                    'characteristics': profile.characteristics,\n                    'segments': []\n                }\n                \n                # Add segments\n                for segment in profile.speech_segments:\n                    segment_data = {\n                        'start_time': segment.start_time,\n                        'end_time': segment.end_time,\n                        'duration': segment.duration,\n                        'confidence': segment.confidence\n                    }\n                    speaker_data['segments'].append(segment_data)\n                    \n                speakers_data.append(speaker_data)\n                \n            # Generate timeline\n            timeline = self._generate_timeline(speaker_profiles)\n            \n            # Statistics\n            statistics = {\n                'total_speakers': len(speaker_profiles),\n                'total_speech_time': total_speech_time,\n                'speech_coverage': (total_speech_time / total_duration) * 100 if total_duration > 0 else 0,\n                'dominant_speaker': speaker_profiles[0].speaker_id if speaker_profiles else None,\n                'speaker_distribution': {f\"Speaker {p.speaker_id + 1}\": p.speech_percentage for p in speaker_profiles}\n            }\n            \n            return {\n                'speakers': speakers_data,\n                'timeline': timeline,\n                'statistics': statistics,\n                'total_duration': total_duration\n            }\n            \n        except Exception as e:\n            logger.error(f\"Error generating results: {e}\")\n            return self._empty_result()\n            \n    def _generate_timeline(self, speaker_profiles, resolution=1.0):\n        \"\"\"Generate speaker timeline dengan resolusi tertentu\"\"\"\n        try:\n            if not speaker_profiles:\n                return []\n                \n            # Get total duration\n            max_end_time = max(\n                max(seg.end_time for seg in profile.speech_segments) \n                for profile in speaker_profiles\n            )\n            \n            timeline = []\n            \n            # Generate timeline points\n            for t in np.arange(0, max_end_time, resolution):\n                active_speakers = []\n                \n                for profile in speaker_profiles:\n                    for segment in profile.speech_segments:\n                        if segment.start_time <= t < segment.end_time:\n                            active_speakers.append({\n                                'speaker_id': profile.speaker_id,\n                                'confidence': segment.confidence\n                            })\n                            break  # Found active segment for this speaker\n                            \n                timeline_point = {\n                    'timestamp': t,\n                    'active_speakers': active_speakers\n                }\n                \n                timeline.append(timeline_point)\n                \n            return timeline\n            \n        except Exception as e:\n            logger.error(f\"Error generating timeline: {e}\")\n            return []\n            \n    def _empty_result(self):\n        \"\"\"Return empty result structure\"\"\"\n        return {\n            'speakers': [],\n            'timeline': [],\n            'statistics': {\n                'total_speakers': 0,\n                'total_speech_time': 0,\n                'speech_coverage': 0,\n                'dominant_speaker': None,\n                'speaker_distribution': {}\n            },\n            'total_duration': 0\n        }\n        \n    def save_diarization_results(self, results, output_path):\n        \"\"\"Save diarization results ke file\"\"\"\n        try:\n            with open(output_path, 'w', encoding='utf-8') as f:\n                json.dump(results, f, indent=2, ensure_ascii=False)\n                \n            logger.info(f\"Diarization results saved to {output_path}\")\n            \n        except Exception as e:\n            logger.error(f\"Error saving diarization results: {e}\")\n\n# Test function\nif __name__ == \"__main__\":\n    # Test speaker diarization\n    diarizer = SpeakerDiarization()\n    \n    def test_progress(progress, message):\n        print(f\"Progress: {progress}% - {message}\")\n    \n    print(\"Speaker Diarization module loaded successfully\")\n    print(f\"Pyannote available: {PYANNOTE_AVAILABLE}\")\n    print(f\"SpeechBrain available: {SPEECHBRAIN_AVAILABLE}\")\n    \n    # Test dengan sample video (uncomment untuk testing)\n    # video_path = \"test_video.mp4\"\n    # results = diarizer.identify_speakers(video_path, test_progress)\n    # \n    # print(f\"\\nDetected {len(results['speakers'])} speakers:\")\n    # for speaker in results['speakers']:\n    #     print(f\"- {speaker['name']}: {speaker['speech_percentage']:.1f}% speaking time\")