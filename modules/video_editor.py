#!/usr/bin/env python3\n\"\"\"\nVideo Editor Module\nMenggabungkan semua hasil AI analysis menjadi video final dengan:\n- Auto-clipping moment terbaik\n- Watermark overlay\n- Subtitle embedding\n- Podcast mode (split atas-bawah)\n- Face tracking crop\n\"\"\"\n\nimport cv2\nimport numpy as np\nfrom pathlib import Path\nimport logging\nfrom typing import List, Dict, Tuple, Optional, Union\nfrom dataclasses import dataclass\nimport json\nfrom moviepy.editor import (\n    VideoFileClip, AudioFileClip, TextClip, CompositeVideoClip,\n    ImageClip, concatenate_videoclips, vfx, afx\n)\nfrom moviepy.video.fx import resize, crop\nfrom PIL import Image, ImageDraw, ImageFont\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom datetime import datetime\nimport threading\nimport queue\n\n# Setup logging\nlogging.basicConfig(level=logging.INFO)\nlogger = logging.getLogger(__name__)\n\n@dataclass\nclass EditingOptions:\n    \"\"\"Data class untuk editing options\"\"\"\n    # Clipping options\n    auto_clip_moments: bool = True\n    max_clips: int = 5\n    min_clip_duration: float = 10.0\n    max_clip_duration: float = 60.0\n    \n    # Watermark options\n    watermark_path: Optional[str] = None\n    watermark_position: str = 'bottom-right'  # 'top-left', 'top-right', 'bottom-left', 'bottom-right', 'center'\n    watermark_opacity: float = 0.8\n    watermark_scale: float = 0.1  # Percentage of video size\n    \n    # Subtitle options\n    embed_subtitles: bool = True\n    subtitle_style: Dict = None\n    \n    # Podcast mode options\n    podcast_mode: bool = False\n    split_speakers: bool = True\n    face_crop_padding: float = 0.2\n    \n    # Output options\n    output_quality: str = '720p'\n    output_format: str = 'mp4'\n    fps: int = 30\n    audio_bitrate: str = '128k'\n    video_bitrate: str = '2000k'\n    \nclass VideoEditor:\n    def __init__(self, output_dir=None, temp_dir=None):\n        \"\"\"Initialize video editor\"\"\"\n        self.output_dir = Path(output_dir) if output_dir else Path(__file__).parent.parent / \"output\"\n        self.temp_dir = Path(temp_dir) if temp_dir else Path(__file__).parent.parent / \"temp\"\n        \n        self.output_dir.mkdir(exist_ok=True)\n        self.temp_dir.mkdir(exist_ok=True)\n        \n        # Quality settings\n        self.quality_settings = {\n            '480p': {'height': 480, 'width': 854},\n            '720p': {'height': 720, 'width': 1280},\n            '1080p': {'height': 1080, 'width': 1920},\n            '1440p': {'height': 1440, 'width': 2560},\n            '4K': {'height': 2160, 'width': 3840}\n        }\n        \n    def process_video(self, video_path, analysis_results, progress_callback=None):\n        \"\"\"\n        Main function untuk memproses video dengan semua AI analysis results\n        \n        Args:\n            video_path: Path ke video original\n            analysis_results: Dict dengan hasil dari semua AI modules\n            progress_callback: Function untuk progress updates\n            \n        Returns:\n            List of output file paths\n        \"\"\"\n        try:\n            logger.info(f\"Starting video processing: {video_path}\")\n            \n            if progress_callback:\n                progress_callback(5, \"Memuat video dan hasil analisis...\")\n                \n            # Extract analysis results\n            moments = analysis_results.get('moments', [])\n            face_data = analysis_results.get('face_data', {})\n            speaker_data = analysis_results.get('speaker_data', {})\n            subtitle_data = analysis_results.get('subtitle_data', {})\n            \n            # Get options\n            options = analysis_results.get('options', EditingOptions())\n            \n            # Load original video\n            original_video = VideoFileClip(video_path)\n            \n            output_files = []\n            \n            if progress_callback:\n                progress_callback(15, \"Menghasilkan clips dari moment terbaik...\")\n                \n            # Generate clips dari best moments\n            if options.auto_clip_moments and moments:\n                clips = self._create_moment_clips(\n                    original_video, moments, options, progress_callback\n                )\n                output_files.extend(clips)\n                \n            if progress_callback:\n                progress_callback(40, \"Memproses podcast mode...\")\n                \n            # Generate podcast mode video\n            if options.podcast_mode:\n                podcast_video = self._create_podcast_mode(\n                    original_video, face_data, speaker_data, options, progress_callback\n                )\n                if podcast_video:\n                    output_files.append(podcast_video)\n                    \n            if progress_callback:\n                progress_callback(65, \"Menambahkan subtitle dan watermark...\")\n                \n            # Create full video dengan enhancements\n            enhanced_video = self._create_enhanced_video(\n                original_video, subtitle_data, options, progress_callback\n            )\n            if enhanced_video:\n                output_files.append(enhanced_video)\n                \n            if progress_callback:\n                progress_callback(90, \"Generating video highlights reel...\")\n                \n            # Create highlights reel\n            if moments:\n                highlights_reel = self._create_highlights_reel(\n                    original_video, moments, subtitle_data, options, progress_callback\n                )\n                if highlights_reel:\n                    output_files.append(highlights_reel)\n                    \n            # Cleanup\n            original_video.close()\n            \n            if progress_callback:\n                progress_callback(100, f\"Video processing selesai - {len(output_files)} file dibuat\")\n                \n            logger.info(f\"Video processing complete. Generated {len(output_files)} files\")\n            return output_files\n            \n        except Exception as e:\n            logger.error(f\"Error processing video: {e}\")\n            return []\n            \n    def _create_moment_clips(self, video, moments, options, progress_callback=None):\n        \"\"\"Create individual clips dari moment terbaik\"\"\"\n        try:\n            output_files = []\n            \n            # Sort moments by score\n            sorted_moments = sorted(moments, key=lambda x: x.get('score', 0), reverse=True)\n            \n            # Limit number of clips\n            clips_to_create = min(len(sorted_moments), options.max_clips)\n            \n            for i, moment in enumerate(sorted_moments[:clips_to_create]):\n                try:\n                    start_time = moment['start_time']\n                    end_time = moment['end_time']\n                    duration = end_time - start_time\n                    \n                    # Skip jika duration tidak sesuai\n                    if duration < options.min_clip_duration or duration > options.max_clip_duration:\n                        continue\n                        \n                    # Extract clip\n                    clip = video.subclip(start_time, end_time)\n                    \n                    # Apply enhancements\n                    if options.watermark_path:\n                        clip = self._add_watermark(clip, options)\n                        \n                    # Generate output filename\n                    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n                    output_filename = f\"moment_clip_{i+1}_{timestamp}.{options.output_format}\"\n                    output_path = self.output_dir / output_filename\n                    \n                    # Export clip\n                    clip.write_videofile(\n                        str(output_path),\n                        fps=options.fps,\n                        bitrate=options.video_bitrate,\n                        audio_bitrate=options.audio_bitrate,\n                        verbose=False,\n                        logger=None\n                    )\n                    \n                    output_files.append(str(output_path))\n                    clip.close()\n                    \n                    if progress_callback:\n                        progress = 15 + ((i + 1) / clips_to_create) * 25\n                        progress_callback(progress, f\"Clip {i+1}/{clips_to_create} selesai\")\n                        \n                except Exception as e:\n                    logger.warning(f\"Error creating clip {i+1}: {e}\")\n                    continue\n                    \n            return output_files\n            \n        except Exception as e:\n            logger.error(f\"Error creating moment clips: {e}\")\n            return []\n            \n    def _create_podcast_mode(self, video, face_data, speaker_data, options, progress_callback=None):\n        \"\"\"Create podcast-style split video (atas-bawah)\"\"\"\n        try:\n            if not face_data.get('tracks') or not speaker_data.get('speakers'):\n                logger.warning(\"Insufficient data for podcast mode\")\n                return None\n                \n            # Get main speakers\n            main_speakers = face_data.get('main_speakers', [])\n            if len(main_speakers) < 2:\n                logger.warning(\"Need at least 2 speakers for podcast mode\")\n                return None\n                \n            # Get video dimensions\n            width, height = video.size\n            \n            # Calculate split dimensions\n            split_height = height // 2\n            \n            # Create clips untuk each speaker\n            speaker_clips = []\n            \n            for i, speaker in enumerate(main_speakers[:2]):  # Max 2 speakers\n                # Get face tracking data untuk crop coordinates\n                face_id = speaker['face_id']\n                \n                # Create cropped video focused on speaker\n                speaker_clip = self._create_speaker_focused_clip(\n                    video, face_id, face_data, split_height, width, options\n                )\n                \n                if speaker_clip:\n                    speaker_clips.append(speaker_clip)\n                    \n            if len(speaker_clips) < 2:\n                logger.warning(\"Could not create clips for both speakers\")\n                return None\n                \n            # Combine clips vertically (atas-bawah)\n            final_clip = CompositeVideoClip([\n                speaker_clips[0].set_position(('center', 0)),\n                speaker_clips[1].set_position(('center', split_height))\n            ], size=(width, height))\n            \n            # Add watermark jika specified\n            if options.watermark_path:\n                final_clip = self._add_watermark(final_clip, options)\n                \n            # Generate output filename\n            timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n            output_filename = f\"podcast_mode_{timestamp}.{options.output_format}\"\n            output_path = self.output_dir / output_filename\n            \n            # Export\n            final_clip.write_videofile(\n                str(output_path),\n                fps=options.fps,\n                bitrate=options.video_bitrate,\n                audio_bitrate=options.audio_bitrate,\n                verbose=False,\n                logger=None\n            )\n            \n            # Cleanup\n            for clip in speaker_clips:\n                clip.close()\n            final_clip.close()\n            \n            return str(output_path)\n            \n        except Exception as e:\n            logger.error(f\"Error creating podcast mode: {e}\")\n            return None\n            \n    def _create_speaker_focused_clip(self, video, face_id, face_data, target_height, target_width, options):\n        \"\"\"Create video clip focused on specific speaker\"\"\"\n        try:\n            # Find face track untuk speaker\n            face_track = None\n            for track in face_data.get('tracks', []):\n                if track['face_id'] == face_id:\n                    face_track = track\n                    break\n                    \n            if not face_track or not face_track.get('timeline'):\n                return None\n                \n            # Calculate average face position\n            timeline = face_track['timeline']\n            \n            # Get bounding boxes untuk calculate crop area\n            bboxes = [point['bounding_box'] for point in timeline]\n            \n            if not bboxes:\n                return None\n                \n            # Calculate average crop area\n            avg_x = np.mean([bbox[0] for bbox in bboxes])\n            avg_y = np.mean([bbox[1] for bbox in bboxes])\n            avg_width = np.mean([bbox[2] for bbox in bboxes])\n            avg_height = np.mean([bbox[3] for bbox in bboxes])\n            \n            # Add padding\n            padding_x = avg_width * options.face_crop_padding\n            padding_y = avg_height * options.face_crop_padding\n            \n            # Calculate crop coordinates\n            crop_x1 = max(0, avg_x - padding_x)\n            crop_y1 = max(0, avg_y - padding_y)\n            crop_x2 = min(video.w, avg_x + avg_width + padding_x)\n            crop_y2 = min(video.h, avg_y + avg_height + padding_y)\n            \n            crop_width = crop_x2 - crop_x1\n            crop_height = crop_y2 - crop_y1\n            \n            # Crop video\n            cropped = video.fx(crop, x1=crop_x1, y1=crop_y1, x2=crop_x2, y2=crop_y2)\n            \n            # Resize untuk fit target dimensions\n            resized = cropped.fx(resize, height=target_height, width=target_width)\n            \n            return resized\n            \n        except Exception as e:\n            logger.error(f\"Error creating speaker focused clip: {e}\")\n            return None\n            \n    def _create_enhanced_video(self, video, subtitle_data, options, progress_callback=None):\n        \"\"\"Create enhanced version of full video dengan subtitle dan watermark\"\"\"\n        try:\n            enhanced = video.copy()\n            \n            # Add subtitles jika available\n            if options.embed_subtitles and subtitle_data.get('segments'):\n                enhanced = self._add_subtitles_to_video(enhanced, subtitle_data, options)\n                \n            # Add watermark\n            if options.watermark_path:\n                enhanced = self._add_watermark(enhanced, options)\n                \n            # Apply quality settings\n            quality = self.quality_settings.get(options.output_quality, self.quality_settings['720p'])\n            enhanced = enhanced.fx(resize, height=quality['height'], width=quality['width'])\n            \n            # Generate output filename\n            timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n            output_filename = f\"enhanced_video_{timestamp}.{options.output_format}\"\n            output_path = self.output_dir / output_filename\n            \n            # Export\n            enhanced.write_videofile(\n                str(output_path),\n                fps=options.fps,\n                bitrate=options.video_bitrate,\n                audio_bitrate=options.audio_bitrate,\n                verbose=False,\n                logger=None\n            )\n            \n            enhanced.close()\n            return str(output_path)\n            \n        except Exception as e:\n            logger.error(f\"Error creating enhanced video: {e}\")\n            return None\n            \n    def _create_highlights_reel(self, video, moments, subtitle_data, options, progress_callback=None):\n        \"\"\"Create highlights reel dari top moments\"\"\"\n        try:\n            if not moments:\n                return None\n                \n            # Sort moments dan ambil top moments\n            sorted_moments = sorted(moments, key=lambda x: x.get('score', 0), reverse=True)\n            top_moments = sorted_moments[:min(len(sorted_moments), 10)]  # Max 10 moments\n            \n            # Create clips dari moments\n            highlight_clips = []\n            \n            for i, moment in enumerate(top_moments):\n                start_time = moment['start_time']\n                end_time = moment['end_time']\n                \n                # Limit duration untuk highlights\n                max_duration = 15.0  # 15 seconds max per highlight\n                if end_time - start_time > max_duration:\n                    end_time = start_time + max_duration\n                    \n                clip = video.subclip(start_time, end_time)\n                \n                # Add title overlay\n                title = f\"Highlight {i+1}\"\n                title_clip = TextClip(\n                    title,\n                    fontsize=30,\n                    color='white',\n                    font='Arial-Bold'\n                ).set_duration(2).set_position(('center', 50))\n                \n                clip_with_title = CompositeVideoClip([clip, title_clip])\n                highlight_clips.append(clip_with_title)\n                \n            if not highlight_clips:\n                return None\n                \n            # Concatenate all highlights\n            highlights_reel = concatenate_videoclips(highlight_clips, method=\"compose\")\n            \n            # Add watermark\n            if options.watermark_path:\n                highlights_reel = self._add_watermark(highlights_reel, options)\n                \n            # Generate output filename\n            timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n            output_filename = f\"highlights_reel_{timestamp}.{options.output_format}\"\n            output_path = self.output_dir / output_filename\n            \n            # Export\n            highlights_reel.write_videofile(\n                str(output_path),\n                fps=options.fps,\n                bitrate=options.video_bitrate,\n                audio_bitrate=options.audio_bitrate,\n                verbose=False,\n                logger=None\n            )\n            \n            # Cleanup\n            for clip in highlight_clips:\n                clip.close()\n            highlights_reel.close()\n            \n            return str(output_path)\n            \n        except Exception as e:\n            logger.error(f\"Error creating highlights reel: {e}\")\n            return None\n            \n    def _add_watermark(self, video, options):\n        \"\"\"Add watermark overlay ke video\"\"\"\n        try:\n            if not options.watermark_path or not Path(options.watermark_path).exists():\n                return video\n                \n            # Load watermark image\n            watermark = ImageClip(options.watermark_path)\n            \n            # Scale watermark\n            watermark_width = int(video.w * options.watermark_scale)\n            watermark = watermark.fx(resize, width=watermark_width)\n            \n            # Set opacity\n            watermark = watermark.set_opacity(options.watermark_opacity)\n            \n            # Set position\n            position = self._get_watermark_position(options.watermark_position, video.w, video.h, watermark.w, watermark.h)\n            watermark = watermark.set_position(position).set_duration(video.duration)\n            \n            # Composite\n            return CompositeVideoClip([video, watermark])\n            \n        except Exception as e:\n            logger.error(f\"Error adding watermark: {e}\")\n            return video\n            \n    def _get_watermark_position(self, position_str, video_w, video_h, watermark_w, watermark_h):\n        \"\"\"Get watermark position coordinates\"\"\"\n        margin = 20\n        \n        positions = {\n            'top-left': (margin, margin),\n            'top-right': (video_w - watermark_w - margin, margin),\n            'bottom-left': (margin, video_h - watermark_h - margin),\n            'bottom-right': (video_w - watermark_w - margin, video_h - watermark_h - margin),\n            'center': ('center', 'center')\n        }\n        \n        return positions.get(position_str, positions['bottom-right'])\n        \n    def _add_subtitles_to_video(self, video, subtitle_data, options):\n        \"\"\"Add subtitles overlay ke video\"\"\"\n        try:\n            segments = subtitle_data.get('segments', [])\n            if not segments:\n                return video\n                \n            subtitle_clips = []\n            \n            for segment in segments:\n                start_time = segment['start_time']\n                end_time = segment['end_time']\n                text = segment['text']\n                \n                # Create text clip\n                txt_clip = TextClip(\n                    text,\n                    fontsize=options.subtitle_style.get('font_size', 20) if options.subtitle_style else 20,\n                    color=options.subtitle_style.get('color', 'white') if options.subtitle_style else 'white',\n                    font='Arial',\n                    stroke_color='black',\n                    stroke_width=2\n                ).set_start(start_time).set_end(end_time)\n                \n                # Set position\n                position = options.subtitle_style.get('position', 'bottom') if options.subtitle_style else 'bottom'\n                if position == 'bottom':\n                    txt_clip = txt_clip.set_position(('center', video.h - 80))\n                elif position == 'top':\n                    txt_clip = txt_clip.set_position(('center', 50))\n                else:\n                    txt_clip = txt_clip.set_position(('center', 'center'))\n                    \n                subtitle_clips.append(txt_clip)\n                \n            # Composite dengan video\n            return CompositeVideoClip([video] + subtitle_clips)\n            \n        except Exception as e:\n            logger.error(f\"Error adding subtitles: {e}\")\n            return video\n            \n    def create_analysis_summary_video(self, analysis_results, output_path=None):\n        \"\"\"Create visualization video dari analysis results\"\"\"\n        try:\n            if not output_path:\n                timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n                output_path = self.output_dir / f\"analysis_summary_{timestamp}.mp4\"\n                \n            # Create visualization frames\n            frames = self._generate_analysis_visualization_frames(analysis_results)\n            \n            if not frames:\n                return None\n                \n            # Convert frames ke video\n            clips = []\n            for frame in frames:\n                clip = ImageClip(frame, duration=3)  # 3 seconds per frame\n                clips.append(clip)\n                \n            if clips:\n                summary_video = concatenate_videoclips(clips, method=\"compose\")\n                summary_video.write_videofile(\n                    str(output_path),\n                    fps=1,  # Low FPS untuk slideshow\n                    verbose=False,\n                    logger=None\n                )\n                summary_video.close()\n                \n            return str(output_path)\n            \n        except Exception as e:\n            logger.error(f\"Error creating analysis summary: {e}\")\n            return None\n            \n    def _generate_analysis_visualization_frames(self, analysis_results):\n        \"\"\"Generate visualization frames untuk analysis summary\"\"\"\n        try:\n            frames = []\n            \n            # Face tracking visualization\n            if analysis_results.get('face_data'):\n                face_frame = self._create_face_analysis_frame(analysis_results['face_data'])\n                if face_frame is not None:\n                    frames.append(face_frame)\n                    \n            # Speaker analysis visualization\n            if analysis_results.get('speaker_data'):\n                speaker_frame = self._create_speaker_analysis_frame(analysis_results['speaker_data'])\n                if speaker_frame is not None:\n                    frames.append(speaker_frame)\n                    \n            # Moments visualization\n            if analysis_results.get('moments'):\n                moments_frame = self._create_moments_analysis_frame(analysis_results['moments'])\n                if moments_frame is not None:\n                    frames.append(moments_frame)\n                    \n            return frames\n            \n        except Exception as e:\n            logger.error(f\"Error generating visualization frames: {e}\")\n            return []\n            \n    def _create_face_analysis_frame(self, face_data):\n        \"\"\"Create visualization frame untuk face analysis\"\"\"\n        try:\n            fig, ax = plt.subplots(1, 1, figsize=(12, 8))\n            \n            # Bar chart of screen time per face\n            tracks = face_data.get('tracks', [])\n            if not tracks:\n                return None\n                \n            face_names = [f\"Face {track['face_id'] + 1}\" for track in tracks]\n            screen_times = [track['screen_time_percentage'] for track in tracks]\n            \n            bars = ax.bar(face_names, screen_times, color='skyblue')\n            ax.set_title('Face Detection Analysis - Screen Time', fontsize=16, fontweight='bold')\n            ax.set_ylabel('Screen Time (%)')\n            ax.set_xlabel('Detected Faces')\n            \n            # Add value labels on bars\n            for bar, value in zip(bars, screen_times):\n                height = bar.get_height()\n                ax.text(bar.get_x() + bar.get_width()/2., height,\n                       f'{value:.1f}%', ha='center', va='bottom')\n                       \n            plt.tight_layout()\n            \n            # Convert ke numpy array\n            fig.canvas.draw()\n            frame = np.frombuffer(fig.canvas.tostring_rgb(), dtype=np.uint8)\n            frame = frame.reshape(fig.canvas.get_width_height()[::-1] + (3,))\n            \n            plt.close(fig)\n            return frame\n            \n        except Exception as e:\n            logger.error(f\"Error creating face analysis frame: {e}\")\n            return None\n            \n    def _create_speaker_analysis_frame(self, speaker_data):\n        \"\"\"Create visualization frame untuk speaker analysis\"\"\"\n        try:\n            fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n            \n            speakers = speaker_data.get('speakers', [])\n            if not speakers:\n                return None\n                \n            # Pie chart of speaking time\n            names = [speaker['name'] for speaker in speakers]\n            percentages = [speaker['speech_percentage'] for speaker in speakers]\n            \n            ax1.pie(percentages, labels=names, autopct='%1.1f%%', startangle=90)\n            ax1.set_title('Speaker Distribution', fontsize=14, fontweight='bold')\n            \n            # Timeline visualization\n            timeline = speaker_data.get('timeline', [])\n            if timeline:\n                timestamps = [point['timestamp'] for point in timeline[:100]]  # Sample points\n                active_speakers = [len(point['active_speakers']) for point in timeline[:100]]\n                \n                ax2.plot(timestamps, active_speakers, linewidth=2, color='green')\n                ax2.set_title('Speaker Activity Over Time', fontsize=14, fontweight='bold')\n                ax2.set_xlabel('Time (seconds)')\n                ax2.set_ylabel('Number of Active Speakers')\n                ax2.grid(True, alpha=0.3)\n                \n            plt.tight_layout()\n            \n            # Convert ke numpy array\n            fig.canvas.draw()\n            frame = np.frombuffer(fig.canvas.tostring_rgb(), dtype=np.uint8)\n            frame = frame.reshape(fig.canvas.get_width_height()[::-1] + (3,))\n            \n            plt.close(fig)\n            return frame\n            \n        except Exception as e:\n            logger.error(f\"Error creating speaker analysis frame: {e}\")\n            return None\n            \n    def _create_moments_analysis_frame(self, moments):\n        \"\"\"Create visualization frame untuk moments analysis\"\"\"\n        try:\n            fig, ax = plt.subplots(1, 1, figsize=(12, 8))\n            \n            if not moments:\n                return None\n                \n            # Sort moments by score\n            sorted_moments = sorted(moments, key=lambda x: x.get('score', 0), reverse=True)\n            top_moments = sorted_moments[:10]  # Top 10 moments\n            \n            # Create bar chart\n            moment_labels = [f\"Moment {i+1}\\n({m['start_time']:.1f}s-{m['end_time']:.1f}s)\" \n                           for i, m in enumerate(top_moments)]\n            scores = [moment['score'] for moment in top_moments]\n            \n            bars = ax.bar(range(len(moment_labels)), scores, color='orange')\n            ax.set_title('Top Moments Analysis - AI Scoring', fontsize=16, fontweight='bold')\n            ax.set_ylabel('AI Score')\n            ax.set_xlabel('Detected Moments')\n            ax.set_xticks(range(len(moment_labels)))\n            ax.set_xticklabels(moment_labels, rotation=45, ha='right')\n            \n            # Add score labels\n            for bar, score in zip(bars, scores):\n                height = bar.get_height()\n                ax.text(bar.get_x() + bar.get_width()/2., height,\n                       f'{score:.3f}', ha='center', va='bottom')\n                       \n            plt.tight_layout()\n            \n            # Convert ke numpy array\n            fig.canvas.draw()\n            frame = np.frombuffer(fig.canvas.tostring_rgb(), dtype=np.uint8)\n            frame = frame.reshape(fig.canvas.get_width_height()[::-1] + (3,))\n            \n            plt.close(fig)\n            return frame\n            \n        except Exception as e:\n            logger.error(f\"Error creating moments analysis frame: {e}\")\n            return None\n\n# Test function\nif __name__ == \"__main__\":\n    # Test video editor\n    editor = VideoEditor()\n    \n    print(\"Video Editor module loaded successfully\")\n    print(f\"Quality settings: {list(editor.quality_settings.keys())}\")\n    \n    # Test dengan sample data (uncomment untuk testing)\n    # sample_analysis = {\n    #     'moments': [\n    #         {'start_time': 10, 'end_time': 30, 'score': 0.8},\n    #         {'start_time': 60, 'end_time': 80, 'score': 0.7}\n    #     ],\n    #     'face_data': {'tracks': []},\n    #     'speaker_data': {'speakers': []},\n    #     'subtitle_data': {'segments': []}\n    # }\n    # \n    # video_path = \"test_video.mp4\"\n    # outputs = editor.process_video(video_path, sample_analysis)\n    # print(f\"Generated {len(outputs)} output files\")